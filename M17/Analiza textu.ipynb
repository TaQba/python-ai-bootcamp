{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091670f5",
   "metadata": {},
   "source": [
    "#  Analiza tematu dokumentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ba964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jtadych/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jtadych/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jtadych/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import itertools\n",
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b9810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dataset = pd.read_csv('spam.csv', encoding=\"ISO-8859-1\", usecols=[0, 1], names=['Spam', 'Text'], skiprows=1)\n",
    "spam_dataset['Spam'] = spam_dataset['Spam'].replace(['ham', 'spam'], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8af4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctation(text):\n",
    "    return ''.join([word for word in text if word not in string.punctuation])\n",
    "spam_dataset['Cleaned_Text'] = spam_dataset['Text'].apply(lambda x: remove_punctation(x))\n",
    "\n",
    "def tokenize(text):\n",
    "\n",
    "    # Usunięcie wielkich liter\n",
    "    clean_text = text.lower()\n",
    "\n",
    "    # Tokenizacja\n",
    "    tokenized_text = nltk.word_tokenize(clean_text)\n",
    "    return tokenized_text\n",
    "spam_dataset['Tokenized_Text'] = spam_dataset['Cleaned_Text'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383aaaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stopwords]\n",
    "spam_dataset['WithoutStop_Text'] = spam_dataset['Tokenized_Text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f13f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    return stemmed_words\n",
    "spam_dataset['Stemmed_Text'] = spam_dataset['WithoutStop_Text'].apply(lambda x: stemming(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d21db216",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmater = nltk.WordNetLemmatizer()\n",
    "def lemmatizing(text):\n",
    "    lemmatized_words = [lemmater.lemmatize(word) for word in text]\n",
    "    return lemmatized_words\n",
    "spam_dataset['Lemmatized_Text'] = spam_dataset['WithoutStop_Text'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57b764b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_v2 = CountVectorizer(max_df=0.1, max_features=500)\n",
    "X_count_v2 = count_v2.fit_transform(spam_dataset['Lemmatized_Text'].apply(lambda x: ' '.join(x)))\n",
    "lda = LatentDirichletAllocation(n_components=7, random_state=2022, learning_method='batch')\n",
    "X_topics = lda.fit_transform(X_count_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19453ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temat: 1\n",
      "lovegooddayhihappy\n",
      "\n",
      "Temat: 2\n",
      "timecomegetyeahgive\n",
      "\n",
      "Temat: 3\n",
      "urltgttxtsendstop\n",
      "\n",
      "Temat: 4\n",
      "goimlorhomeok\n",
      "\n",
      "Temat: 5\n",
      "dontknowimwantlike\n",
      "\n",
      "Temat: 6\n",
      "callfreeokphonetext\n",
      "\n",
      "Temat: 7\n",
      "callnathatspleaseclaim\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_v2.get_feature_names_out()\n",
    "for topic_idx , topic in enumerate(lda.components_):\n",
    "    print('Temat: {}'.format(topic_idx+1))\n",
    "    print('' . join([feature_names[i] for i in topic.argsort()[:-5-1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b3bf823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '100' '1000' '150' '150p' '150ppm' '16' '18' '1st' '2000' '250'\n",
      " '2nd' '500' '5000' '750' '800' '8007' '86688' 'able' 'abt' 'account'\n",
      " 'actually' 'address' 'aft' 'afternoon' 'ah' 'aight' 'already' 'alright'\n",
      " 'also' 'always' 'amp' 'angry' 'another' 'answer' 'anything' 'anyway'\n",
      " 'apply' 'ard' 'around' 'ask' 'asked' 'attempt' 'await' 'award' 'awarded'\n",
      " 'away' 'awesome' 'babe' 'baby' 'back' 'bad' 'beautiful' 'bed' 'believe'\n",
      " 'best' 'better' 'big' 'birthday' 'bit' 'bonus' 'book' 'bored' 'box' 'boy'\n",
      " 'break' 'bring' 'brother' 'bt' 'bus' 'busy' 'buy' 'call' 'called'\n",
      " 'calling' 'came' 'camera' 'cant' 'car' 'card' 'care' 'cash' 'cause'\n",
      " 'chance' 'change' 'charge' 'chat' 'check' 'claim' 'class' 'close' 'club'\n",
      " 'co' 'code' 'collect' 'collection' 'colour' 'come' 'coming' 'contact'\n",
      " 'cool' 'cost' 'could' 'coz' 'credit' 'customer' 'da' 'dad' 'dat' 'date'\n",
      " 'day' 'de' 'dear' 'delivery' 'den' 'detail' 'didnt' 'dinner' 'dis'\n",
      " 'doesnt' 'done' 'dont' 'double' 'draw' 'dream' 'drink' 'drive' 'driving'\n",
      " 'drop' 'dude' 'dun' 'dunno' 'early' 'easy' 'eat' 'either' 'else' 'email'\n",
      " 'end' 'enjoy' 'enough' 'entry' 'eve' 'even' 'evening' 'ever' 'every'\n",
      " 'everyone' 'everything' 'face' 'family' 'feel' 'feeling' 'find' 'fine'\n",
      " 'finish' 'finished' 'first' 'food' 'forgot' 'free' 'friend' 'friendship'\n",
      " 'fuck' 'full' 'fun' 'game' 'get' 'getting' 'gift' 'girl' 'give' 'go'\n",
      " 'god' 'goin' 'going' 'gon' 'good' 'got' 'gr8' 'great' 'guaranteed' 'gud'\n",
      " 'guess' 'guy' 'haf' 'haha' 'hair' 'half' 'hand' 'happen' 'happy' 'hav'\n",
      " 'havent' 'he' 'head' 'hear' 'heart' 'hello' 'help' 'hey' 'hi' 'holiday'\n",
      " 'home' 'hope' 'hot' 'hour' 'house' 'hows' 'huh' 'hurt' 'id' 'ill' 'im'\n",
      " 'important' 'ive' 'job' 'join' 'jus' 'juz' 'keep' 'kiss' 'know' 'land'\n",
      " 'landline' 'lar' 'last' 'late' 'later' 'latest' 'leave' 'leaving' 'left'\n",
      " 'leh' 'lei' 'lesson' 'let' 'liao' 'life' 'like' 'line' 'little' 'live'\n",
      " 'lol' 'long' 'look' 'looking' 'lor' 'lose' 'lot' 'love' 'ltdecimalgt'\n",
      " 'ltgt' 'lunch' 'luv' 'made' 'mail' 'make' 'making' 'man' 'many' 'match'\n",
      " 'mate' 'may' 'maybe' 'mean' 'meet' 'meeting' 'message' 'might' 'min'\n",
      " 'mind' 'minute' 'miss' 'missed' 'missing' 'mob' 'mobile' 'mom' 'money'\n",
      " 'month' 'morning' 'movie' 'msg' 'much' 'mum' 'music' 'must' 'na' 'name'\n",
      " 'need' 'network' 'never' 'new' 'next' 'nice' 'night' 'nite' 'noe' 'nokia'\n",
      " 'nothing' 'number' 'offer' 'office' 'oh' 'ok' 'okay' 'okie' 'old' 'one'\n",
      " 'online' 'orange' 'order' 'oso' 'pa' 'pain' 'part' 'pay' 'people' 'per'\n",
      " 'person' 'phone' 'pic' 'pick' 'place' 'plan' 'play' 'player' 'please'\n",
      " 'pls' 'plus' 'plz' 'po' 'point' 'post' 'pound' 'price' 'princess'\n",
      " 'private' 'prize' 'probably' 'problem' 'pub' 'put' 'question' 'quite'\n",
      " 'rate' 'reach' 'ready' 'real' 'really' 'receive' 'remember' 'reply'\n",
      " 'right' 'ring' 'ringtone' 'rite' 'room' 'run' 'sad' 'sae' 'said' 'sat'\n",
      " 'saw' 'say' 'saying' 'sch' 'school' 'second' 'see' 'selected' 'send'\n",
      " 'sent' 'service' 'set' 'sexy' 'shall' 'shes' 'shit' 'shop' 'shopping'\n",
      " 'show' 'si' 'simple' 'since' 'sir' 'sleep' 'sm' 'smile' 'smiling' 'smoke'\n",
      " 'someone' 'something' 'soon' 'sorry' 'sound' 'speak' 'special' 'start'\n",
      " 'stay' 'still' 'stop' 'story' 'stuff' 'sure' 'sweet' 'take' 'talk' 'tc'\n",
      " 'tell' 'test' 'text' 'thank' 'thanks' 'thanx' 'thats' 'there' 'thing'\n",
      " 'think' 'thinking' 'thk' 'though' 'thought' 'til' 'till' 'time' 'tmr'\n",
      " 'today' 'together' 'told' 'tomorrow' 'tone' 'tonight' 'took' 'top' 'tot'\n",
      " 'town' 'tried' 'trip' 'true' 'try' 'trying' 'tv' 'two' 'txt' 'uk'\n",
      " 'update' 'ur' 'urgent' 'use' 'valid' 'video' 'voucher' 'wait' 'waiting'\n",
      " 'wake' 'walk' 'wan' 'want' 'wanted' 'wat' 'watch' 'watching' 'way' 'week'\n",
      " 'weekend' 'weekly' 'well' 'wen' 'went' 'whats' 'wid' 'wif' 'wife' 'win'\n",
      " 'wish' 'without' 'wk' 'wont' 'word' 'work' 'working' 'world' 'worry'\n",
      " 'wot' 'would' 'xmas' 'xxx' 'ya' 'yeah' 'year' 'yes' 'yesterday' 'yet'\n",
      " 'yo' 'youre' 'yr' 'yup' 'ìï']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e51b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
